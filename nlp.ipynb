{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41045821",
   "metadata": {},
   "source": [
    "## NLP Praktisches Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcbaf4",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/Jona-Bach/llm-notebooks/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b956d14",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c567c9",
   "metadata": {},
   "source": [
    "**Klassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca1e72",
   "metadata": {},
   "source": [
    "Das englische Modell Schaut nur nach Positiv oder Negativ\n",
    "\n",
    "Das multilinguale Modell gibt 1 - 5 Sterne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "english_model = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "multi_lingual_model = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "classifier = pipeline(\"sentiment-analysis\", model= english_model, framework=\"pt\")\n",
    "multi_classifier = pipeline(\"sentiment-analysis\", model= multi_lingual_model, framework=\"pt\")\n",
    "\n",
    "text = \"Trees are green\"\n",
    "text2 = \"I hate this boat\"\n",
    "\n",
    "result = classifier(text)\n",
    "result_multi = multi_classifier(text2)\n",
    "print(text, result)\n",
    "print(text2, result_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fea88b",
   "metadata": {},
   "source": [
    "**German**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install germansentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c885bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from germansentiment import SentimentModel\n",
    "\n",
    "sent_model = SentimentModel()\n",
    "\n",
    "text = [\"Der Tag ist grün und die Sterne lila\"] # Der Text muss in einer Liste übergeben werden, es können auch mehrere Sätze analysiert werden\n",
    "\n",
    "result, probability = sent_model.predict_sentiment(text, output_probabilities=True)\n",
    "print(result, probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d1afe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad5087",
   "metadata": {},
   "source": [
    "### Thema 4: **Aktuelle Tools**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88cc19",
   "metadata": {},
   "source": [
    "### Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96618b80",
   "metadata": {},
   "source": [
    "#### Google Flan (Text2Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "# model_id = \"google/flan-t5-small\" # Dümmer aber schneller\n",
    "filenames = [\"pytorch_model.bin\",\"config.json\",\"generation_config.json\",\"special_tokens_map.json\",\"spiece.model\",\"tokenizer_config.json\"]\n",
    "for file in filenames:\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename = file,\n",
    "    )\n",
    "    print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c61a2",
   "metadata": {},
   "source": [
    "**Achtung das laden der Pipeline kann etwas dauern!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b1a44",
   "metadata": {},
   "source": [
    "Am besten Englisch verwenden. Text2Text Modelle sind gut für Aufgaben wie Zusammenfassungen, Übersetzungen oder Aufgabenlösung!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f850e0",
   "metadata": {},
   "source": [
    "**e** drücken zum beenden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38affee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer)\n",
    "\n",
    "while True:\n",
    "    question = input(\"Give me a Task(e zum beenden): \")\n",
    "    answer = generator(question)\n",
    "    print(answer[0][\"generated_text\"])\n",
    "\n",
    "    if question.lower() == \"e\":\n",
    "        print(\"Beenden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcfd0d",
   "metadata": {},
   "source": [
    "#### Tiny Llama (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94900461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "filenames = [\n",
    "    \"model.safetensors\",\n",
    "    \"config.json\",\n",
    "    \"eval_results.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"generation_config.json\"\n",
    "]\n",
    "\n",
    "for file in filenames:\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=file,\n",
    "    )\n",
    "    print(f\"{file} -> {downloaded_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "question = \"Q: Where is the Buckingham Palace? \\nA:\"\n",
    "\n",
    "response = generator(\n",
    "    question,\n",
    "    max_new_tokens=50,          # Begrenzung!\n",
    "    do_sample=True,              # zufälligere Antworten\n",
    "    temperature=0.7,             # Kreativität\n",
    "    top_p=0.9                    # typische Sampling-Kombi\n",
    ")\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c11669",
   "metadata": {},
   "source": [
    "### Ollama ( + Langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07b3b1",
   "metadata": {},
   "source": [
    "Wir verwenden hier das *llama3.2:3b* Modell von Meta\n",
    "\n",
    "Dies kann man sich runterladen durch: **Ollama pull llama:3.2:3b** (Ollama muss vorher installiert werden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21b5f6",
   "metadata": {},
   "source": [
    "**e** zum beenden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_name = \"llama3.2:3b\" # 3 Millarden Parameter\n",
    "llm = OllamaLLM(model=model_name)\n",
    "\n",
    "template = \"Beantworte diese Frage direkt und Präzise: \\n{frage}\"\n",
    "prompt = PromptTemplate(input_variables=[\"frage\"], template=template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "frage = \"Wo steht der Buckingham Palace?\"\n",
    "\n",
    "antwort = chain.invoke({\"frage\": frage})\n",
    "print(antwort)\n",
    "\n",
    "# while True:\n",
    "\n",
    "#     input_user = input(\"Stelle eine Frage: \")\n",
    "\n",
    "#     if input_user.lower() == \"e\":\n",
    "#         print(\"Beenden\")\n",
    "#         break\n",
    "#     antwort = chain.invoke({\"frage\": input_user})\n",
    "#     print(antwort)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
